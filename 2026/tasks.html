<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Tasks | CMRxRecon2026</title>
    <meta name="description" content="website of CMRx series challenges">
    <meta name="generator" content="VitePress v2.0.0-alpha.15">
    <link rel="preload stylesheet" href="/2026/assets/style.N5yNrAsh.css" as="style">
    <link rel="preload stylesheet" href="/2026/vp-icons.css" as="style">
    
    <script type="module" src="/2026/assets/app.DgKj0Kpq.js"></script>
    <link rel="preload" href="/2026/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/2026/assets/chunks/theme.BV-jWiY0.js">
    <link rel="modulepreload" href="/2026/assets/chunks/framework.CFBmzcfj.js">
    <link rel="modulepreload" href="/2026/assets/tasks.md.CfdmZz7E.lean.js">
    <link rel="icon" type="image/png" href="/2026/public/logo-combined.png">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-Y1FZN1BJN8"></script>
    <script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y1FZN1BJN8");</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-aa5637b3><!--[--><!--]--><!--[--><span tabindex="-1" data-v-f023c09b></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-f023c09b>Skip to content</a><!--]--><!----><header class="VPNav" data-v-aa5637b3 data-v-5b7fda00><div class="VPNavBar" data-v-5b7fda00 data-v-787d5905><div class="wrapper" data-v-787d5905><div class="container" data-v-787d5905><div class="title" data-v-787d5905><div class="VPNavBarTitle" data-v-787d5905 data-v-016ef833><a class="title" href="/2026/" data-v-016ef833><!--[--><!--]--><!--[--><img class="VPImage logo" src="/2026/logo-combined.png" alt data-v-2aac4770><!--]--><span data-v-016ef833>CMRxRecon2026</span><!--[--><!--]--></a></div></div><div class="content" data-v-787d5905><div class="content-body" data-v-787d5905><!--[--><!--]--><div class="VPNavBarSearch search" data-v-787d5905><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-787d5905 data-v-8fa2167c><span id="main-nav-aria-label" class="visually-hidden" data-v-8fa2167c> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/2026/" tabindex="0" data-v-8fa2167c data-v-691eb544><!--[--><span data-v-691eb544>Home</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/2026/data.html" tabindex="0" data-v-8fa2167c data-v-691eb544><!--[--><span data-v-691eb544>Data</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink active" href="/2026/tasks.html" tabindex="0" data-v-8fa2167c data-v-691eb544><!--[--><span data-v-691eb544>Tasks</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/2026/join-the-challenge.html" tabindex="0" data-v-8fa2167c data-v-691eb544><!--[--><span data-v-691eb544>Join the challenge</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-8fa2167c data-v-53e4ba04><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-53e4ba04><span class="text" data-v-53e4ba04><!----><span data-v-53e4ba04>Submission</span><span class="vpi-chevron-down text-icon" data-v-53e4ba04></span></span></button><div class="menu" data-v-53e4ba04><div class="VPMenu" data-v-53e4ba04 data-v-a919ad15><div class="items" data-v-a919ad15><!--[--><!--[--><div class="VPMenuLink" data-v-a919ad15 data-v-24a2cfbc><a class="VPLink link" href="/2026/submission-task.html" data-v-24a2cfbc><!--[--><span data-v-24a2cfbc>Task Submission</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a919ad15 data-v-24a2cfbc><a class="VPLink link" href="/2026/submission-stacom-workshop-paper.html" data-v-24a2cfbc><!--[--><span data-v-24a2cfbc>Stacom workshop paper</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/2026/sponsors.html" tabindex="0" data-v-8fa2167c data-v-691eb544><!--[--><span data-v-691eb544>Sponsors</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/2026/organizers.html" tabindex="0" data-v-8fa2167c data-v-691eb544><!--[--><span data-v-691eb544>Organizers</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/2026/faq.html" tabindex="0" data-v-8fa2167c data-v-691eb544><!--[--><span data-v-691eb544>FAQ</span><!--]--></a><!--]--><!--]--></nav><!----><!----><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-787d5905 data-v-abca0262 data-v-c4aadf9f><!--[--><a class="VPSocialLink no-icon" href="https://github.com/CmrxRecon" aria-label="github" target="_blank" rel="me noopener" data-v-c4aadf9f data-v-2098176d><span class="vpi-social-github"></span></a><a class="VPSocialLink no-icon" href="https://twitter.com/CMRxRecon" aria-label="twitter" target="_blank" rel="me noopener" data-v-c4aadf9f data-v-2098176d><span class="vpi-social-twitter"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-787d5905 data-v-df5658be data-v-53e4ba04><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-53e4ba04><span class="vpi-more-horizontal icon" data-v-53e4ba04></span></button><div class="menu" data-v-53e4ba04><div class="VPMenu" data-v-53e4ba04 data-v-a919ad15><!----><!--[--><!--[--><!----><!----><div class="group" data-v-df5658be><div class="item social-links" data-v-df5658be><div class="VPSocialLinks social-links-list" data-v-df5658be data-v-c4aadf9f><!--[--><a class="VPSocialLink no-icon" href="https://github.com/CmrxRecon" aria-label="github" target="_blank" rel="me noopener" data-v-c4aadf9f data-v-2098176d><span class="vpi-social-github"></span></a><a class="VPSocialLink no-icon" href="https://twitter.com/CMRxRecon" aria-label="twitter" target="_blank" rel="me noopener" data-v-c4aadf9f data-v-2098176d><span class="vpi-social-twitter"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-787d5905 data-v-95e8efd7><span class="container" data-v-95e8efd7><span class="top" data-v-95e8efd7></span><span class="middle" data-v-95e8efd7></span><span class="bottom" data-v-95e8efd7></span></span></button></div></div></div></div><div class="divider" data-v-787d5905><div class="divider-line" data-v-787d5905></div></div></div><!----></header><div class="VPLocalNav empty fixed" data-v-aa5637b3 data-v-cba36b6a><div class="container" data-v-cba36b6a><!----><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-cba36b6a data-v-5284fa83><button data-v-5284fa83>Return to top</button><!----></div></div></div><!----><div class="VPContent" id="VPContent" data-v-aa5637b3 data-v-e2017ced><div class="VPDoc has-aside" data-v-e2017ced data-v-5137736c><!--[--><!--]--><div class="container" data-v-5137736c><div class="aside" data-v-5137736c><div class="aside-curtain" data-v-5137736c></div><div class="aside-container" data-v-5137736c><div class="aside-content" data-v-5137736c><div class="VPDocAside" data-v-5137736c data-v-5de03f5c><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-5de03f5c data-v-5889524e><div class="content" data-v-5889524e><div class="outline-marker" data-v-5889524e></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-5889524e>On this page</div><ul class="VPDocOutlineItem root" data-v-5889524e data-v-9d53e7a0><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-5de03f5c></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-5137736c><div class="content-container" data-v-5137736c><!--[--><!--]--><main class="main" data-v-5137736c><div style="position:relative;" class="vp-doc _2026_tasks" data-v-5137736c><div><h1 id="tasks" tabindex="-1">Tasks <a class="header-anchor" href="#tasks" aria-label="Permalink to ‚ÄúTasks‚Äù">‚Äã</a></h1><!----><p>The CMRxRecon2026 challenge includes two regular tasks and two special tasks. The tasks are awarded independently, so each team can choose to participate in any one of them. For each task, participants can submit one model each (which can be four different models, or they can submit just one model, but they must submit it separately for each task). Please note that for the four tasks, the training dataset we provide is the same; however, the validation and test datasets may be different for each of the four tasks.</p><h2 id="objective" tabindex="-1">Objective <a class="header-anchor" href="#objective" aria-label="Permalink to ‚ÄúObjective‚Äù">‚Äã</a></h2><p>To evaluate the models&#39; capability for accurate reconstruction under ultra-high acceleration (Regular Task 1), rapid processing with limited computing resources (Regular Task 2), generalizability across new clinical sites and diseases (Special Task 1), and adaptability to different anatomical regions (Special Task 2).</p><hr><p>üèÜ <strong>Regular Task 1: Accurate Reconstruction under High Acceleration</strong></p><p>This track targets the fundamental challenge of recovering high-fidelity 4D Flow information from severely undersampled k-space data. Participants are tasked with developing advanced reconstruction algorithms capable of operating at extreme acceleration factors, directly addressing the current barrier of lengthy scan times.</p><ul><li><p><strong>Goal:</strong> Reconstruct 4D Flow data under extreme acceleration factors (10x ‚Äì 50x).</p></li><li><p><strong>Core Challenge:</strong> The primary hurdle is to accurately recover not only the anatomical structure (represented by the magnitude image) but, critically, also the precise phase information that encodes the blood flow velocity. Errors in phase reconstruction directly translate to inaccurate hemodynamic parameters, undermining clinical utility.</p></li><li><p><strong>Evaluation Metrics:</strong> Submissions will undergo a comprehensive assessment of reconstruction fidelity.</p><ul><li><strong>Magnitude Image Quality:</strong> Evaluated using standard metrics such as Normalized Root Mean Square Error (nRMSE) and Structural Similarity Index Measure (SSIM), focusing on the overall visual integrity and detail preservation of the reconstructed images.</li><li><strong>Flow Velocity Field Accuracy:</strong> Evaluated using Relative Velocity Error (RelErr) and Angular Error (AngErr), comparing reconstructed velocity vectors against ground truth.</li></ul></li><li><p><strong>Ranking Method:</strong></p><ul><li><strong>Per-metric aggregation:</strong> For each submission and each metric (nRMSE, SSIM, RelErr, AngErr), compute the average metric value across all test cases.</li><li><strong>Per-metric ranking:</strong> Rank methods separately for each metric: <ul><li>Lower is better: nRMSE, RelErr, AngErr</li><li>Higher is better: SSIM</li></ul></li><li><strong>Overall ranking:</strong> For each method, sum its ranks across the four metrics to obtain a composite score. Methods with the lowest summed rank are ranked highest overall.</li></ul></li></ul><hr><p>üèÜ <strong>Regular Task 2: Fast Reconstruction under Limited Computing Resources</strong></p><p>This task prioritizes computational efficiency for routine clinical adoption. Participants develop algorithms that minimize reconstruction latency on standard workstation hardware, ensuring 4D Flow MRI is practical for high-throughput diagnostic workflows.</p><ul><li><p><strong>Goal:</strong> Achieve the fastest possible reconstruction using limited computational resources (standardized on a single NVIDIA A6000 48G GPU).</p></li><li><p><strong>Core Challenge:</strong> Reconstructing high-dimensional 4D Flow data within the computational limits of a single GPU, optimizing reconstruction efficiency while ensuring the diagnostic integrity of blood flow quantification.</p></li><li><p><strong>Evaluation Process:</strong> The evaluation for Task 2 follows a two-stage process designed to prioritize computational speed without compromising clinical diagnostic quality.</p><ul><li><p><strong>Stage 1: Quality Qualification (Threshold Baseline)</strong></p><ul><li><strong>Benchmark:</strong> The FlowVN algorithm (Vishnevskiy et al., Nat Mach Intell. 2020) serves as the baseline for acceptable reconstruction quality.</li><li><strong>Evaluation Metric:</strong> Reconstruction quality is assessed using a unified metric, Complex Difference Error ($E_{complex}$), that evaluates the accuracy of the reconstructed complex-valued data, capturing both magnitude and phase information.</li><li><strong>Qualification Requirement:</strong> Only algorithms achieving an $E_{complex}$ value equal to or lower than the FlowVN baseline qualify for the final ranking.</li></ul></li><li><p><strong>Stage 2: Efficiency Ranking (Final Performance)</strong></p><ul><li><strong>Standardized Platform:</strong> After meeting the reconstruction quality threshold, algorithms are ranked based on their computational efficiency. Reconstruction time per case, measured in seconds, is evaluated on a single NVIDIA A6000 GPU with 48 GB VRAM.</li><li><strong>Evaluation Metric:</strong> Average Reconstruction Time ‚Äì the mean end-to-end duration required to transform raw k-space input into final reconstructed images. The algorithm that delivers the shortest processing time per case achieves the highest rank.</li></ul></li></ul></li><li><p><strong>Ranking Method:</strong></p><ul><li><strong>Quality gate (eligibility):</strong> Submissions are first checked against a reconstruction-quality threshold benchmarked to FlowVN. Only methods that achieve quality equal to or better than FlowVN are eligible for ranking.</li><li><strong>Time-based ranking:</strong> Among eligible submissions, rank methods by reconstruction time (seconds per case) measured on the standardized hardware platform (single NVIDIA A6000, 48GB). Shorter time ranks higher.</li></ul></li></ul><hr><p>üèÜ <strong>Special Task 1: Generalizability Across New Sites and Diseases</strong></p><p>Real-world clinical data often varies significantly across different hospitals, scanner models, and patient populations, especially in challenging disease states. This task investigates the robustness and generalizability of your reconstruction algorithms when faced with unseen data characteristics.</p><ul><li><p><strong>Goal:</strong> Develop reconstruction models that maintain high accuracy and quality when applied to 4D Flow data acquired from new, unseen clinical sites (e.g., different MRI scanner models) and/or new disease pathologies not explicitly represented in the primary training set.</p></li><li><p><strong>Core Challenge:</strong> Overcoming domain shift. Algorithms must demonstrate resilience to variations in:</p><ul><li>Signal-to-noise ratio</li><li>Coil sensitivities</li><li>Acquisition parameters</li><li>Pathological manifestations<br> without retraining or fine-tuning on the new domain. This pushes the boundaries of robustness and true clinical applicability.</li></ul></li><li><p><strong>Evaluation Metrics:</strong> Submissions will undergo a comprehensive assessment of reconstruction fidelity:</p><ul><li><strong>Magnitude Image Quality:</strong> Evaluated using standard metrics such as Normalized Root Mean Square Error (nRMSE) and Structural Similarity Index Measure (SSIM), focusing on the overall visual integrity and detail preservation of the reconstructed images.</li><li><strong>Flow Velocity Field Accuracy:</strong> Evaluated using Relative Velocity Error (RelErr) and Angular Error (AngErr), comparing reconstructed velocity vectors against ground truth.</li></ul></li><li><p><strong>Ranking Method:</strong></p><ul><li><strong>Per-metric aggregation:</strong> For each submission and each metric (nRMSE, SSIM, RelErr, AngErr), compute the average metric value across all test cases.</li><li><strong>Per-metric ranking:</strong> Rank methods separately for each metric: <ul><li>Lower is better: nRMSE, RelErr, AngErr</li><li>Higher is better: SSIM</li></ul></li><li><strong>Overall ranking:</strong> For each method, sum its ranks across the four metrics to obtain a composite score. Methods with the lowest summed rank are ranked highest overall.</li></ul></li></ul><hr><p>üèÜ <strong>Special Task 2: Generalizability Across Different Anatomical Regions</strong></p><p>While the primary regular tasks focus on aortic 4D Flow, the principles of blood flow imaging and reconstruction are broadly applicable to other critical vascular beds. This task explores the transferability of algorithms developed for aortic data to entirely different anatomical regions within the cardiovascular system.</p><ul><li><p><strong>Goal:</strong> Evaluate how effectively reconstruction algorithms, primarily trained on aortic 4D Flow data, can generalize to other important vascular regions such as:</p><ul><li>Portal venous system</li><li>Renal arteries</li><li>Cerebral vessels</li><li>Cervical vessels<br> without significant architectural changes or specific training on these new regions.</li></ul></li><li><p><strong>Core Challenge:</strong> Adapting to new anatomical geometries, flow patterns, and potential variations in image contrast and resolution inherent to different vascular territories. This tests the fundamental understanding of flow dynamics learned by the model, rather than just aortic-specific features.</p></li><li><p><strong>Evaluation Metrics:</strong> Submissions will undergo a comprehensive assessment of reconstruction fidelity:</p><ul><li><strong>Magnitude Image Quality:</strong> Evaluated using standard metrics such as Normalized Root Mean Square Error (nRMSE) and Structural Similarity Index Measure (SSIM), focusing on the overall visual integrity and detail preservation of the reconstructed images.</li><li><strong>Flow Velocity Field Accuracy:</strong> Evaluated using Relative Velocity Error (RelErr) and Angular Error (AngErr), comparing reconstructed velocity vectors against ground truth.</li></ul></li><li><p><strong>Ranking Method:</strong></p><ul><li><strong>Per-metric aggregation:</strong> For each submission and each metric (nRMSE, SSIM, RelErr, AngErr), compute the average metric value across all test cases.</li><li><strong>Per-metric ranking:</strong> Rank methods separately for each metric: <ul><li>Lower is better: nRMSE, RelErr, AngErr</li><li>Higher is better: SSIM</li></ul></li><li><strong>Overall ranking:</strong> For each method, sum its ranks across the four metrics to obtain a composite score. Methods with the lowest summed rank are ranked highest overall.</li></ul></li></ul><hr><h2 id="üéñÔ∏è-awards" tabindex="-1">üéñÔ∏è Awards <a class="header-anchor" href="#üéñÔ∏è-awards" aria-label="Permalink to ‚ÄúüéñÔ∏è Awards‚Äù">‚Äã</a></h2><p>The top 5 winners in each task will receive monetary awards. The bonus distribution plan is shown in the table below.</p><!----><p>All submissions will be reported in the leaderboard. Each participating team can engage in any tasks or all four tasks. Prize-winning methods will be announced publicly as part of a scientific session at the MICCAI annual meeting.</p><hr><h4 id="task-submission" tabindex="-1">Task Submission <a class="header-anchor" href="#task-submission" aria-label="Permalink to ‚ÄúTask Submission‚Äù">‚Äã</a></h4><ul><li><strong>Validation submission tutorial:</strong> <a href="https://www.synapse.org/Synapse:syn64545434/wiki/637610" target="_blank" rel="noreferrer">Synapse Wiki</a></li><li><strong>Test submission tutorial:</strong> <a href="https://www.synapse.org/Synapse:syn64545434/wiki/637611" target="_blank" rel="noreferrer">Synapse Wiki</a></li></ul><h4 id="challenge-platform" tabindex="-1">Challenge Platform <a class="header-anchor" href="#challenge-platform" aria-label="Permalink to ‚ÄúChallenge Platform‚Äù">‚Äã</a></h4><ul><li>Hosted on <strong>Synapse</strong> platform: <a href="https://www.synapse.org/Synapse:syn64545434/wiki/" target="_blank" rel="noreferrer">https://www.synapse.org/Synapse:syn64545434/wiki/</a></li></ul><hr><h2 id="principal-of-participation" tabindex="-1">Principal of Participation <a class="header-anchor" href="#principal-of-participation" aria-label="Permalink to ‚ÄúPrincipal of Participation‚Äù">‚Äã</a></h2><!----><p>Note: Participants are not required to upload the complete training code. But teams willing to upload the original training code will be automatically entered into the code-sharing pool.</p><hr><h2 id="timeline" tabindex="-1">Timeline <a class="header-anchor" href="#timeline" aria-label="Permalink to ‚ÄúTimeline‚Äù">‚Äã</a></h2><p>The schedule of the challenge is as follows. All deadlines are Pacific Standard Time (PST +11:59).</p><div style="max-width:700px;margin-top:1rem;"><div style="padding:10px 14px;background-color:#f5f7fa;"><strong>[Feb. 01, 2026]</strong> Website opens for registration </div><div style="padding:10px 14px;background-color:#ffffff;"><strong>[Mar. 01, 2026]</strong> Release training data and validation data </div><div style="padding:10px 14px;background-color:#f5f7fa;"><strong>[May. 01, 2026]</strong> Submission system opens for validation </div><div style="padding:10px 14px;background-color:#ffffff;"><strong>[Jul. 01, 2026]</strong> Submission system opens for testing </div><div style="padding:10px 14px;background-color:#f5f7fa;"><strong>[Jul. 30, 2026]</strong> STACOM paper submission deadline </div><div style="padding:10px 14px;background-color:#ffffff;"><strong>[Aug. 20, 2026]</strong> Testing docker submission deadline </div><div style="padding:10px 14px;background-color:#f5f7fa;"><strong>[Oct. 08, 2026]</strong> Release final results during the MICCAI annual meeting </div></div><hr><h2 id="rules" tabindex="-1">Rules <a class="header-anchor" href="#rules" aria-label="Permalink to ‚ÄúRules‚Äù">‚Äã</a></h2><ol><li>It should be restricted to the data provided by the current and previous CMRxRecon challenge as well as data from the ‚ÄòfastMRI‚Äô challenge (the most related public dataset), under the terms and conditions associated with the data usage.</li><li>For each task, participants are allowed to train only one model to reconstruct various images at the aforementioned different undersampling scenarios.</li></ol><hr></div></div></main><footer class="VPDocFooter" data-v-5137736c data-v-30c9eb43><!--[--><!--]--><!----><!----></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter" data-v-aa5637b3 data-v-aca8c503><div class="container" data-v-aca8c503><p class="message" data-v-aca8c503>
    <center>
      <img src="/2026/public/alllogos.png" style="height:80px; margin-bottom: 10px;" />
    </center>
    <div>Released under the MIT License, powered by 
      <a target="_blank" rel="noopener noreferrer" href="https://vitepress.dev/">VitePress</a>.
    </div>
  </p><p class="copyright" data-v-aca8c503>Copyright ¬© 2026-present CMRxRecon Team</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"aVX5Zvxx\",\"data.md\":\"MgF6zINW\",\"faq.md\":\"BF9-XEsI\",\"index.md\":\"CA_42myj\",\"index_.md\":\"DX7Hl3hh\",\"join-the-challenge.md\":\"BeZPqTkm\",\"markdown-examples.md\":\"FFqp-SS_\",\"navigation.md\":\"cMoPaoQD\",\"organizers.md\":\"CrwjaoDr\",\"sponsors.md\":\"CdyrYRs_\",\"submission-stacom-workshop-paper.md\":\"C2B45FqJ\",\"submission-task.md\":\"BGA-BKKH\",\"tasks.md\":\"CfdmZz7E\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"CMRxRecon2026\",\"description\":\"website of CMRx series challenges\",\"base\":\"/2026/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":false,\"themeConfig\":{\"logo\":\"/logo-combined.png\",\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"Data\",\"link\":\"/data\"},{\"text\":\"Tasks\",\"link\":\"/tasks\"},{\"text\":\"Join the challenge\",\"link\":\"/join-the-challenge\"},{\"text\":\"Submission\",\"items\":[{\"text\":\"Task Submission\",\"link\":\"/submission-task\"},{\"text\":\"Stacom workshop paper\",\"link\":\"/submission-stacom-workshop-paper\"}]},{\"text\":\"Sponsors\",\"link\":\"/sponsors\"},{\"text\":\"Organizers\",\"link\":\"/organizers\"},{\"text\":\"FAQ\",\"link\":\"/faq\"}],\"sidebar\":[],\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/CmrxRecon\"},{\"icon\":\"twitter\",\"link\":\"https://twitter.com/CMRxRecon\"}],\"footer\":{\"message\":\"\\n    <center>\\n      <img src=\\\"/2026/public/alllogos.png\\\" style=\\\"height:80px; margin-bottom: 10px;\\\" />\\n    </center>\\n    <div>Released under the MIT License, powered by \\n      <a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"https://vitepress.dev/\\\">VitePress</a>.\\n    </div>\\n  \",\"copyright\":\"Copyright ¬© 2026-present CMRxRecon Team\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false,\"additionalConfig\":{}}");</script>
    
  </body>
</html>