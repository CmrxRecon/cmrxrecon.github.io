<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Join the Challenge | CMRxRecon2026</title>
    <meta name="description" content="website of CMRx series challenges">
    <meta name="generator" content="VitePress v2.0.0-alpha.15">
    <link rel="preload stylesheet" href="/2026/assets/style.N5yNrAsh.css" as="style">
    <link rel="preload stylesheet" href="/2026/vp-icons.css" as="style">
    
    <script type="module" src="/2026/assets/app.DgKj0Kpq.js"></script>
    <link rel="preload" href="/2026/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/2026/assets/chunks/theme.BV-jWiY0.js">
    <link rel="modulepreload" href="/2026/assets/chunks/framework.CFBmzcfj.js">
    <link rel="modulepreload" href="/2026/assets/join-the-challenge.md.BeZPqTkm.lean.js">
    <link rel="icon" type="image/png" href="/2026/public/logo-combined.png">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-Y1FZN1BJN8"></script>
    <script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y1FZN1BJN8");</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-aa5637b3><!--[--><!--]--><!--[--><span tabindex="-1" data-v-f023c09b></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-f023c09b>Skip to content</a><!--]--><!----><header class="VPNav" data-v-aa5637b3 data-v-5b7fda00><div class="VPNavBar" data-v-5b7fda00 data-v-787d5905><div class="wrapper" data-v-787d5905><div class="container" data-v-787d5905><div class="title" data-v-787d5905><div class="VPNavBarTitle" data-v-787d5905 data-v-016ef833><a class="title" href="/2026/" data-v-016ef833><!--[--><!--]--><!--[--><img class="VPImage logo" src="/2026/logo-combined.png" alt data-v-2aac4770><!--]--><span data-v-016ef833>CMRxRecon2026</span><!--[--><!--]--></a></div></div><div class="content" data-v-787d5905><div class="content-body" data-v-787d5905><!--[--><!--]--><div class="VPNavBarSearch search" data-v-787d5905><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-787d5905 data-v-8fa2167c><span id="main-nav-aria-label" class="visually-hidden" data-v-8fa2167c> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/2026/" tabindex="0" data-v-8fa2167c data-v-691eb544><!--[--><span data-v-691eb544>Home</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/2026/data.html" tabindex="0" data-v-8fa2167c data-v-691eb544><!--[--><span data-v-691eb544>Data</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/2026/tasks.html" tabindex="0" data-v-8fa2167c data-v-691eb544><!--[--><span data-v-691eb544>Tasks</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink active" href="/2026/join-the-challenge.html" tabindex="0" data-v-8fa2167c data-v-691eb544><!--[--><span data-v-691eb544>Join the challenge</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-8fa2167c data-v-53e4ba04><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-53e4ba04><span class="text" data-v-53e4ba04><!----><span data-v-53e4ba04>Submission</span><span class="vpi-chevron-down text-icon" data-v-53e4ba04></span></span></button><div class="menu" data-v-53e4ba04><div class="VPMenu" data-v-53e4ba04 data-v-a919ad15><div class="items" data-v-a919ad15><!--[--><!--[--><div class="VPMenuLink" data-v-a919ad15 data-v-24a2cfbc><a class="VPLink link" href="/2026/submission-task.html" data-v-24a2cfbc><!--[--><span data-v-24a2cfbc>Task Submission</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a919ad15 data-v-24a2cfbc><a class="VPLink link" href="/2026/submission-stacom-workshop-paper.html" data-v-24a2cfbc><!--[--><span data-v-24a2cfbc>Stacom workshop paper</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/2026/sponsors.html" tabindex="0" data-v-8fa2167c data-v-691eb544><!--[--><span data-v-691eb544>Sponsors</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/2026/organizers.html" tabindex="0" data-v-8fa2167c data-v-691eb544><!--[--><span data-v-691eb544>Organizers</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/2026/faq.html" tabindex="0" data-v-8fa2167c data-v-691eb544><!--[--><span data-v-691eb544>FAQ</span><!--]--></a><!--]--><!--]--></nav><!----><!----><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-787d5905 data-v-abca0262 data-v-c4aadf9f><!--[--><a class="VPSocialLink no-icon" href="https://github.com/CmrxRecon" aria-label="github" target="_blank" rel="me noopener" data-v-c4aadf9f data-v-2098176d><span class="vpi-social-github"></span></a><a class="VPSocialLink no-icon" href="https://twitter.com/CMRxRecon" aria-label="twitter" target="_blank" rel="me noopener" data-v-c4aadf9f data-v-2098176d><span class="vpi-social-twitter"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-787d5905 data-v-df5658be data-v-53e4ba04><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-53e4ba04><span class="vpi-more-horizontal icon" data-v-53e4ba04></span></button><div class="menu" data-v-53e4ba04><div class="VPMenu" data-v-53e4ba04 data-v-a919ad15><!----><!--[--><!--[--><!----><!----><div class="group" data-v-df5658be><div class="item social-links" data-v-df5658be><div class="VPSocialLinks social-links-list" data-v-df5658be data-v-c4aadf9f><!--[--><a class="VPSocialLink no-icon" href="https://github.com/CmrxRecon" aria-label="github" target="_blank" rel="me noopener" data-v-c4aadf9f data-v-2098176d><span class="vpi-social-github"></span></a><a class="VPSocialLink no-icon" href="https://twitter.com/CMRxRecon" aria-label="twitter" target="_blank" rel="me noopener" data-v-c4aadf9f data-v-2098176d><span class="vpi-social-twitter"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-787d5905 data-v-95e8efd7><span class="container" data-v-95e8efd7><span class="top" data-v-95e8efd7></span><span class="middle" data-v-95e8efd7></span><span class="bottom" data-v-95e8efd7></span></span></button></div></div></div></div><div class="divider" data-v-787d5905><div class="divider-line" data-v-787d5905></div></div></div><!----></header><div class="VPLocalNav empty fixed" data-v-aa5637b3 data-v-cba36b6a><div class="container" data-v-cba36b6a><!----><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-cba36b6a data-v-5284fa83><button data-v-5284fa83>Return to top</button><!----></div></div></div><!----><div class="VPContent" id="VPContent" data-v-aa5637b3 data-v-e2017ced><div class="VPDoc has-aside" data-v-e2017ced data-v-5137736c><!--[--><!--]--><div class="container" data-v-5137736c><div class="aside" data-v-5137736c><div class="aside-curtain" data-v-5137736c></div><div class="aside-container" data-v-5137736c><div class="aside-content" data-v-5137736c><div class="VPDocAside" data-v-5137736c data-v-5de03f5c><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-5de03f5c data-v-5889524e><div class="content" data-v-5889524e><div class="outline-marker" data-v-5889524e></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-5889524e>On this page</div><ul class="VPDocOutlineItem root" data-v-5889524e data-v-9d53e7a0><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-5de03f5c></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-5137736c><div class="content-container" data-v-5137736c><!--[--><!--]--><main class="main" data-v-5137736c><div style="position:relative;" class="vp-doc _2026_join-the-challenge" data-v-5137736c><div><h1 id="join-the-challenge" tabindex="-1">Join The Challenge! <a class="header-anchor" href="#join-the-challenge" aria-label="Permalink to “Join The Challenge!”">​</a></h1><h3 id="join-the-challenge-1" tabindex="-1">Join the challenge! <a class="header-anchor" href="#join-the-challenge-1" aria-label="Permalink to “Join the challenge!”">​</a></h3><ol><li>Sign up and apply to join the challenge on the Synapse platform: <a href="https://www.synapse.org/Synapse:syn64545434/wiki/" target="_blank" rel="noreferrer">Synapse Project Page</a> (<code>https://www.synapse.org/Synapse:syn64545434/wiki/</code>)</li><li>Submit your team information here: <a href="https://www.wjx.top/vm/rkAd42X.aspx#" target="_blank" rel="noreferrer">WJX Form</a> (<code>https://www.wjx.top/vm/rkAd42X.aspx#</code>)</li></ol><h3 id="download-the-data" tabindex="-1">Download the data <a class="header-anchor" href="#download-the-data" aria-label="Permalink to “Download the data”">​</a></h3><p>Download data here: <a href="https://www.synapse.org/Synapse:syn64545434/wiki/638361" target="_blank" rel="noreferrer">Synapse Data</a> (<code>https://www.synapse.org/Synapse:syn64545434/wiki/638361</code>)</p><h3 id="train-the-model" tabindex="-1">Train the model <a class="header-anchor" href="#train-the-model" aria-label="Permalink to “Train the model”">​</a></h3><p>Participants are expected to train models in their local computational environments and submit docker containers on the Synapse platform.<br> A leaderboard will be maintained on the Synapse platform during the validation phase.</p><hr><h3 id="code-availability" tabindex="-1">Code Availability <a class="header-anchor" href="#code-availability" aria-label="Permalink to “Code Availability”">​</a></h3><p>We provide the code to facilitate the use of the 4D Flow data we release: <a href="https://github.com/CmrxRecon/CMRx4DFlow2026" target="_blank" rel="noreferrer">GitHub Repository</a> (<code>https://github.com/CmrxRecon/CMRx4DFlow2026</code>).</p><p>A brief description of the provided package is as follows:</p><ul><li><code>ChallengeDataFormat/</code>: Provides an overview of the 4D Flow MRI dataset and a detailed description of the data format used in the challenge.</li><li><code>CMRx4DFlowMaskGeneration/</code>: Contains code to generate undersampling masks for training, validation, and test data.</li><li><code>CMRx4DFlowReconDemo/</code>: Includes demos for undersampling, Compressed Sensing reconstruction, FlowVN reconstruction, post-processing, and evaluation.</li><li><code>Submission/</code>: Provides instructions for submitting your final results.</li></ul><hr><h2 id="evaluation-platform" tabindex="-1">Evaluation platform <a class="header-anchor" href="#evaluation-platform" aria-label="Permalink to “Evaluation platform”">​</a></h2><p>Validation of the received docker will be performed on a cloud server with the following configuration:</p><ul><li><strong>OS:</strong> Linux (RockyOS 9)</li><li><strong>CPU:</strong> 2.0GHz, 112 cores</li><li><strong>RAM:</strong> 64 GB</li><li><strong>GPU:</strong> A6000 (48 GB VRAM, single GPU)</li><li><strong>GPU Driver Version:</strong> 550</li><li><strong>CUDA Version:</strong> 12.4</li><li><strong>Time Limitation:</strong> 20 hours/team for each task</li></ul><hr><h3 id="publication-references" tabindex="-1">Publication References <a class="header-anchor" href="#publication-references" aria-label="Permalink to “Publication References”">​</a></h3><p>You are free to use and/or refer to the CMRx4DFlow2026 challenge and datasets in your own research after the embargo period (Dec. 2026), provided that you cite the following manuscripts:</p><p><strong>References of the CMRx Series Dataset</strong></p><ol><li>Wang C, Lyu J, Wang S, et al. <em>CMRxRecon: A publicly available k-space dataset and benchmark to advance deep learning for cardiac MRI</em>. Scientific Data, 2024, 11(1): 687. <a href="https://doi.org/10.1038/s41597-024-03525-4" target="_blank" rel="noreferrer">DOI</a></li><li>Wang Z, Wang F, Qin C, et al. <em>CMRxRecon2024: A Multimodality, Multiview k-Space Dataset Boosting Universal Machine Learning for Accelerated Cardiac MRI</em>, Radiology: Artificial Intelligence, 2025, 7(2): e240443. <a href="https://doi.org/10.1148/ryai.240443" target="_blank" rel="noreferrer">DOI</a></li><li>Wang Z, Huang M, Shi Z, et al. <em>Enabling Ultra-Fast Cardiovascular Imaging Across Heterogeneous Clinical Environments with a Generalist Foundation Model and Multimodal Database</em>. arXiv preprint arXiv:2512.21652, 2025. <a href="https://doi.org/10.48550/arXiv.2512.21652" target="_blank" rel="noreferrer">DOI</a></li></ol><p><strong>CMRx Series Challenge Summary Papers</strong></p><ol><li>Lyu J, Qin C, Wang S, et al. <em>The state-of-the-art in cardiac MRI reconstruction: Results of the CMRxRecon challenge in MICCAI 2023</em>. Medical Image Analysis, 2025, 101: 103485. <a href="https://doi.org/10.1016/j.media.2025.103485" target="_blank" rel="noreferrer">DOI</a></li><li>Wang K, Qin C, Shi Z, et al. <em>Extreme cardiac MRI analysis under respiratory motion: Results of the CMRxMotion Challenge</em>. Medical Image Analysis, 2025: 103883. <a href="https://doi.org/10.1016/j.media.2025.103883" target="_blank" rel="noreferrer">DOI</a></li><li>Wang F, Wang Z, Li Y, et al. <em>Towards Modality-and Sampling-Universal Learning Strategies for Accelerating Cardiovascular Imaging: Summary of the CMRxRecon2024 Challenge</em>. IEEE Transactions on Medical Imaging, 2025. <a href="https://doi.org/10.1109/TMI.2025.3641610" target="_blank" rel="noreferrer">DOI</a></li></ol><p><strong>References for Previously Developed Algorithms by Organizers</strong></p><ol><li>Wang C, Li Y, Lv J, et al. <em>Recommendation for Cardiac Magnetic Resonance Imaging-Based Phenotypic Study: Imaging Part</em>. Phenomics. 2021, 1(4): 151-170. <a href="https://doi.org/10.1007/s43657-021-00018-x" target="_blank" rel="noreferrer">DOI</a></li><li>Lyu J, Li G, Wang C, et al. <em>Region-focused multi-view transformer-based generative adversarial network for cardiac cine MRI reconstruction</em>. Medical Image Analysis, 2023: 102760. <a href="https://doi.org/10.1016/j.media.2023.102760" target="_blank" rel="noreferrer">DOI</a></li><li>Lyu J, Tian Y, Cai Q, et al. <em>Adaptive channel-modulated personalized federated learning for magnetic resonance image reconstruction</em>. Computers in Biology and Medicine, 2023, 165: 107330. <a href="https://doi.org/10.1016/j.compbiomed.2023.107330" target="_blank" rel="noreferrer">DOI</a></li><li>Wang Z, Qian C, Guo D, et al. <em>One-dimensional Deep Low-rank and Sparse Network for Accelerated MRI</em>, IEEE Transactions on Medical Imaging, 42: 79-90, 2023. <a href="https://doi.org/10.1109/TMI.2022.3203312" target="_blank" rel="noreferrer">DOI</a></li><li>Qin C, Schlemper J, Caballero J, et al. <em>Convolutional recurrent neural networks for dynamic MR image reconstruction</em>. IEEE Transactions on Medical Imaging, 2018, 38(1): 280-290. <a href="https://doi.org/10.1109/TMI.2018.2863670" target="_blank" rel="noreferrer">DOI</a></li><li>Lyu J, Wang S, Tian Y, et al. <em>STADNet: Spatial-Temporal Attention-Guided Dual-Path Network for cardiac cine MRI super-resolution</em>. Medical Image Analysis, 2024, 94: 103142. <a href="https://doi.org/10.1016/j.media.2024.103142" target="_blank" rel="noreferrer">DOI</a></li><li>Wang Z, Xiao M, Zhou Y, et al. <em>Deep separable spatiotemporal learning for fast dynamic cardiac MRI</em>. IEEE Transactions on Biomedical Engineering, 2025. <a href="https://doi.org/10.1109/TBME.2025.3574090" target="_blank" rel="noreferrer">DOI</a></li><li>Huang J, Yang L, Wang F, et al. <em>Enhancing global sensitivity and uncertainty quantification in medical image reconstruction with Monte Carlo arbitrary-masked mamba</em>. Medical Image Analysis, 2025, 99: 103334. <a href="https://doi.org/10.1016/j.media.2024.103334" target="_blank" rel="noreferrer">DOI</a></li><li>Wang Z, Yu X, Wang C, et al. <em>One for multiple: Physics-informed synthetic data boosts generalizable deep learning for fast MRI reconstruction</em>. Medical Image Analysis, 2025, 103: 103616. <a href="https://doi.org/10.1016/j.media.2025.103616" target="_blank" rel="noreferrer">DOI</a></li><li>Lyu J, Wang G, Wang Z, et al. <em>Diffusion-prior based implicit neural representation for arbitrary-scale cardiac cine MRI super-resolution</em>. Information Fusion, 2025: 103510. <a href="https://doi.org/10.1016/j.inffus.2025.103510" target="_blank" rel="noreferrer">DOI</a></li></ol><p><strong>References of the images cited on this website</strong></p><ol><li><a href="https://commons.wikimedia.org/w/index.php?curid=53001321" target="_blank" rel="noreferrer">Wikimedia</a></li><li>Sandino, Christopher M., et al. <em>Accelerated abdominal 4D flow MRI using 3D golden-angle cones trajectory.</em> Proceedings of the Proc Ann Mtg ISMRM, Honolulu, HI, USA (2017): 22-27.</li><li>Rice J, et al. <em>In Vitro 4D Flow MRI for the Analysis of Aortic Coarctation.</em> Proc. Intl. Soc. Mag. Reson. Med. 30 (2022): 0088. <a href="https://doi.org/10.58530/2022/0088" target="_blank" rel="noreferrer">DOI</a></li><li>Peper, Eva S., et al. <em>10-fold accelerated 4D flow in the carotid arteries at high spatiotemporal resolution in 7 minutes using a novel 15 channel coil.</em> Proceedings of the 24th Annual Meeting of ISMRM, Singapore. 2016.</li></ol></div></div></main><footer class="VPDocFooter" data-v-5137736c data-v-30c9eb43><!--[--><!--]--><!----><!----></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter" data-v-aa5637b3 data-v-aca8c503><div class="container" data-v-aca8c503><p class="message" data-v-aca8c503><center><img src="/2026/alllogos.png" style="height:80px; margin-bottom: 10px;" /></center></p><p class="copyright" data-v-aca8c503><div>Released under the MIT License, powered by<a target="_blank" rel="noopener noreferrer" href="https://vitepress.dev/">VitePress</a>.</div>Copyright © 2026-present CMRx4DFlow2026 Team. </p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"aVX5Zvxx\",\"data.md\":\"MgF6zINW\",\"faq.md\":\"BF9-XEsI\",\"index.md\":\"CA_42myj\",\"index_.md\":\"DX7Hl3hh\",\"join-the-challenge.md\":\"BeZPqTkm\",\"markdown-examples.md\":\"FFqp-SS_\",\"navigation.md\":\"cMoPaoQD\",\"organizers.md\":\"CrwjaoDr\",\"sponsors.md\":\"CdyrYRs_\",\"submission-stacom-workshop-paper.md\":\"C2B45FqJ\",\"submission-task.md\":\"BGA-BKKH\",\"tasks.md\":\"CfdmZz7E\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"CMRxRecon2026\",\"description\":\"website of CMRx series challenges\",\"base\":\"/2026/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":false,\"themeConfig\":{\"logo\":\"/logo-combined.png\",\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"Data\",\"link\":\"/data\"},{\"text\":\"Tasks\",\"link\":\"/tasks\"},{\"text\":\"Join the challenge\",\"link\":\"/join-the-challenge\"},{\"text\":\"Submission\",\"items\":[{\"text\":\"Task Submission\",\"link\":\"/submission-task\"},{\"text\":\"Stacom workshop paper\",\"link\":\"/submission-stacom-workshop-paper\"}]},{\"text\":\"Sponsors\",\"link\":\"/sponsors\"},{\"text\":\"Organizers\",\"link\":\"/organizers\"},{\"text\":\"FAQ\",\"link\":\"/faq\"}],\"sidebar\":[],\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/CmrxRecon\"},{\"icon\":\"twitter\",\"link\":\"https://twitter.com/CMRxRecon\"}],\"footer\":{\"message\":\"<center><img src=\\\"/2026/alllogos.png\\\" style=\\\"height:80px; margin-bottom: 10px;\\\" /></center>\",\"copyright\":\"<div>Released under the MIT License, powered by<a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"https://vitepress.dev/\\\">VitePress</a>.</div>Copyright © 2026-present CMRx4DFlow2026 Team. \"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false,\"additionalConfig\":{}}");</script>
    
  </body>
</html>