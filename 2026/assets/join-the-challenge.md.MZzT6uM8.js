import{_ as a,c as r,o as n,ah as i}from"./chunks/framework.DwRtsSRk.js";const m=JSON.parse('{"title":"Join the Challenge","description":"","frontmatter":{"title":"Join the Challenge"},"headers":[],"relativePath":"join-the-challenge.md","filePath":"join-the-challenge.md"}'),t={name:"join-the-challenge.md"};function o(l,e,s,d,c,h){return n(),r("div",null,[...e[0]||(e[0]=[i('<h1 id="join-the-challenge" tabindex="-1">Join The Challenge! <a class="header-anchor" href="#join-the-challenge" aria-label="Permalink to “Join The Challenge!”">​</a></h1><h3 id="join-the-challenge-1" tabindex="-1">Join the challenge! <a class="header-anchor" href="#join-the-challenge-1" aria-label="Permalink to “Join the challenge!”">​</a></h3><ol><li>Sign up and apply to join the challenge on the Synapse platform: <a href="https://www.synapse.org/Synapse:syn64545434/wiki/" target="_blank" rel="noreferrer">Synapse Project Page</a> (<code>https://www.synapse.org/Synapse:syn64545434/wiki/</code>)</li><li>Submit your team information here: <a href="https://www.wjx.top/vm/rkAd42X.aspx#" target="_blank" rel="noreferrer">WJX Form</a> (<code>https://www.wjx.top/vm/rkAd42X.aspx#</code>)</li></ol><h3 id="download-the-data" tabindex="-1">Download the data <a class="header-anchor" href="#download-the-data" aria-label="Permalink to “Download the data”">​</a></h3><p>Download mimic data here (for testing)<a href="https://www.synapse.org/Synapse:syn73710887" target="_blank" rel="noreferrer">Data</a> (<code>https://www.synapse.org/Synapse:syn73710887</code>)</p><p>Download full data here (for the challenge) <a href="https://www.synapse.org/Synapse:syn64545434/wiki/638361" target="_blank" rel="noreferrer">Data</a> (<code>https://www.synapse.org/Synapse:syn64545434/wiki/638361</code>)</p><h3 id="train-the-model" tabindex="-1">Train the model <a class="header-anchor" href="#train-the-model" aria-label="Permalink to “Train the model”">​</a></h3><p>Participants are expected to train models in their local computational environments and submit docker containers on the Synapse platform.<br> A leaderboard will be maintained on the Synapse platform during the validation phase.</p><hr><h3 id="code-availability" tabindex="-1">Code Availability <a class="header-anchor" href="#code-availability" aria-label="Permalink to “Code Availability”">​</a></h3><p>We provide the code to facilitate the use of the 4D Flow data we release: <a href="https://github.com/CmrxRecon/CMRx4DFlow2026" target="_blank" rel="noreferrer">GitHub Repository</a> (<code>https://github.com/CmrxRecon/CMRx4DFlow2026</code>).</p><p>A brief description of the provided package is as follows:</p><ul><li><code>ChallengeDataFormat/</code>: Provides an overview of the 4D Flow MRI dataset and a detailed description of the data format used in the challenge.</li><li><code>CMRx4DFlowMaskGeneration/</code>: Contains code to generate undersampling masks for training, validation, and test data.</li><li><code>CMRx4DFlowReconDemo/</code>: Includes demos for undersampling, Compressed Sensing reconstruction, FlowVN reconstruction, post-processing, and evaluation.</li><li><code>Submission/</code>: Provides instructions for submitting your final results.</li></ul><hr><h2 id="evaluation-platform" tabindex="-1">Evaluation platform <a class="header-anchor" href="#evaluation-platform" aria-label="Permalink to “Evaluation platform”">​</a></h2><p>Validation of the received docker will be performed on a cloud server with the following configuration:</p><ul><li><strong>OS:</strong> Linux (RockyOS 9)</li><li><strong>CPU:</strong> 2.0GHz, 112 cores</li><li><strong>RAM:</strong> 64 GB</li><li><strong>GPU:</strong> A6000 (48 GB VRAM, single GPU)</li><li><strong>GPU Driver Version:</strong> 550</li><li><strong>CUDA Version:</strong> 12.4</li><li><strong>Time Limitation:</strong> 20 hours/team for each task</li></ul><hr><h3 id="publication-references" tabindex="-1">Publication References <a class="header-anchor" href="#publication-references" aria-label="Permalink to “Publication References”">​</a></h3><p>You are free to use and/or refer to the CMRx4DFlow2026 challenge and datasets in your own research after the embargo period (Dec. 2026), provided that you cite the following manuscripts:</p><p><strong>References of the CMRx Series Dataset</strong></p><ol><li>Wang C, Lyu J, Wang S, et al. <em>CMRxRecon: A publicly available k-space dataset and benchmark to advance deep learning for cardiac MRI</em>. Scientific Data, 2024, 11(1): 687. <a href="https://doi.org/10.1038/s41597-024-03525-4" target="_blank" rel="noreferrer">DOI</a></li><li>Wang Z, Wang F, Qin C, et al. <em>CMRxRecon2024: A Multimodality, Multiview k-Space Dataset Boosting Universal Machine Learning for Accelerated Cardiac MRI</em>, Radiology: Artificial Intelligence, 2025, 7(2): e240443. <a href="https://doi.org/10.1148/ryai.240443" target="_blank" rel="noreferrer">DOI</a></li><li>Wang Z, Huang M, Shi Z, et al. <em>Enabling Ultra-Fast Cardiovascular Imaging Across Heterogeneous Clinical Environments with a Generalist Foundation Model and Multimodal Database</em>. arXiv preprint arXiv:2512.21652, 2025. <a href="https://doi.org/10.48550/arXiv.2512.21652" target="_blank" rel="noreferrer">DOI</a></li></ol><p><strong>CMRx Series Challenge Summary Papers</strong></p><ol><li>Lyu J, Qin C, Wang S, et al. <em>The state-of-the-art in cardiac MRI reconstruction: Results of the CMRxRecon challenge in MICCAI 2023</em>. Medical Image Analysis, 2025, 101: 103485. <a href="https://doi.org/10.1016/j.media.2025.103485" target="_blank" rel="noreferrer">DOI</a></li><li>Wang K, Qin C, Shi Z, et al. <em>Extreme cardiac MRI analysis under respiratory motion: Results of the CMRxMotion Challenge</em>. Medical Image Analysis, 2025: 103883. <a href="https://doi.org/10.1016/j.media.2025.103883" target="_blank" rel="noreferrer">DOI</a></li><li>Wang F, Wang Z, Li Y, et al. <em>Towards Modality-and Sampling-Universal Learning Strategies for Accelerating Cardiovascular Imaging: Summary of the CMRxRecon2024 Challenge</em>. IEEE Transactions on Medical Imaging, 2025. <a href="https://doi.org/10.1109/TMI.2025.3641610" target="_blank" rel="noreferrer">DOI</a></li></ol><p><strong>References for Previously Developed Algorithms by Organizers</strong></p><ol><li>Wang C, Li Y, Lv J, et al. <em>Recommendation for Cardiac Magnetic Resonance Imaging-Based Phenotypic Study: Imaging Part</em>. Phenomics. 2021, 1(4): 151-170. <a href="https://doi.org/10.1007/s43657-021-00018-x" target="_blank" rel="noreferrer">DOI</a></li><li>Lyu J, Li G, Wang C, et al. <em>Region-focused multi-view transformer-based generative adversarial network for cardiac cine MRI reconstruction</em>. Medical Image Analysis, 2023: 102760. <a href="https://doi.org/10.1016/j.media.2023.102760" target="_blank" rel="noreferrer">DOI</a></li><li>Lyu J, Tian Y, Cai Q, et al. <em>Adaptive channel-modulated personalized federated learning for magnetic resonance image reconstruction</em>. Computers in Biology and Medicine, 2023, 165: 107330. <a href="https://doi.org/10.1016/j.compbiomed.2023.107330" target="_blank" rel="noreferrer">DOI</a></li><li>Wang Z, Qian C, Guo D, et al. <em>One-dimensional Deep Low-rank and Sparse Network for Accelerated MRI</em>, IEEE Transactions on Medical Imaging, 42: 79-90, 2023. <a href="https://doi.org/10.1109/TMI.2022.3203312" target="_blank" rel="noreferrer">DOI</a></li><li>Qin C, Schlemper J, Caballero J, et al. <em>Convolutional recurrent neural networks for dynamic MR image reconstruction</em>. IEEE Transactions on Medical Imaging, 2018, 38(1): 280-290. <a href="https://doi.org/10.1109/TMI.2018.2863670" target="_blank" rel="noreferrer">DOI</a></li><li>Lyu J, Wang S, Tian Y, et al. <em>STADNet: Spatial-Temporal Attention-Guided Dual-Path Network for cardiac cine MRI super-resolution</em>. Medical Image Analysis, 2024, 94: 103142. <a href="https://doi.org/10.1016/j.media.2024.103142" target="_blank" rel="noreferrer">DOI</a></li><li>Wang Z, Xiao M, Zhou Y, et al. <em>Deep separable spatiotemporal learning for fast dynamic cardiac MRI</em>. IEEE Transactions on Biomedical Engineering, 2025. <a href="https://doi.org/10.1109/TBME.2025.3574090" target="_blank" rel="noreferrer">DOI</a></li><li>Huang J, Yang L, Wang F, et al. <em>Enhancing global sensitivity and uncertainty quantification in medical image reconstruction with Monte Carlo arbitrary-masked mamba</em>. Medical Image Analysis, 2025, 99: 103334. <a href="https://doi.org/10.1016/j.media.2024.103334" target="_blank" rel="noreferrer">DOI</a></li><li>Wang Z, Yu X, Wang C, et al. <em>One for multiple: Physics-informed synthetic data boosts generalizable deep learning for fast MRI reconstruction</em>. Medical Image Analysis, 2025, 103: 103616. <a href="https://doi.org/10.1016/j.media.2025.103616" target="_blank" rel="noreferrer">DOI</a></li><li>Lyu J, Wang G, Wang Z, et al. <em>Diffusion-prior based implicit neural representation for arbitrary-scale cardiac cine MRI super-resolution</em>. Information Fusion, 2025: 103510. <a href="https://doi.org/10.1016/j.inffus.2025.103510" target="_blank" rel="noreferrer">DOI</a></li></ol><p><strong>References of the images cited on this website</strong></p><ol><li><a href="https://commons.wikimedia.org/w/index.php?curid=53001321" target="_blank" rel="noreferrer">Wikimedia</a></li><li>Sandino, Christopher M., et al. <em>Accelerated abdominal 4D flow MRI using 3D golden-angle cones trajectory.</em> Proceedings of the Proc Ann Mtg ISMRM, Honolulu, HI, USA (2017): 22-27.</li><li>Rice J, et al. <em>In Vitro 4D Flow MRI for the Analysis of Aortic Coarctation.</em> Proc. Intl. Soc. Mag. Reson. Med. 30 (2022): 0088. <a href="https://doi.org/10.58530/2022/0088" target="_blank" rel="noreferrer">DOI</a></li><li>Peper, Eva S., et al. <em>10-fold accelerated 4D flow in the carotid arteries at high spatiotemporal resolution in 7 minutes using a novel 15 channel coil.</em> Proceedings of the 24th Annual Meeting of ISMRM, Singapore. 2016.</li></ol>',28)])])}const p=a(t,[["render",o]]);export{m as __pageData,p as default};
